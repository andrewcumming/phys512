

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Machine Learning &#8212; PHYS 512 Computational Physics with Applications</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'mllecture';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="ChatGPT ‘Real-Life Example’" href="chatgpt.html" />
    <link rel="prev" title="Lorentzian fit" href="nonlinear_fit.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/plasma.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/plasma.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">About the course</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="outline.html">Course outline</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="basics.html">Getting set up</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming.html">Programming best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="floats.html">Floating point arithmetic</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Manipulating functions</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="derivatives.html">Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="interpolation.html">Interpolation</a></li>
<li class="toctree-l1"><a class="reference internal" href="integration.html">Integration</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="function_solutions.html">Solutions to exercises</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="derivatives_solutions.html">Derivatives exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="interpolation_solutions.html">Interpolation exercises</a></li>
<li class="toctree-l2"><a class="reference internal" href="integration_solutions.html">Integration exercises</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Random numbers</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="random.html">Random numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="lcg.html">Linear Congruential Generators</a></li>
<li class="toctree-l1"><a class="reference internal" href="generating_random.html">Probability distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="monte_carlo_integration.html">Monte Carlo integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="metropolis.html">Metropolis-Hastings algorithm</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="random_solutions.html">Solutions to exercises</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="prob_distributions_solutions.html">Probability distributions Exercise 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="prob_distributions_solutions_part2.html">Probability distributions Part 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="montecarlo_solutions.html">Monte Carlo Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="metropolis_solutions.html">Metropolis-Hastings</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Solving systems of equations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="matrices.html">Matrices in numpy</a></li>

<li class="toctree-l1"><a class="reference internal" href="linear_least_squares.html">Linear least squares</a></li>
<li class="toctree-l1"><a class="reference internal" href="polynomial_fit.html">Polynomial fitting</a></li>
<li class="toctree-l1"><a class="reference internal" href="nonlinear.html">Non-linear least squares</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="solving_solutions.html">Solutions to exercises</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="polynomial_fit_solutions.html">Orthogonal polynomials</a></li>
<li class="toctree-l2"><a class="reference internal" href="nonlinear_fit.html">Lorentzian fit</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Machine Learning</a></li>

<li class="toctree-l1"><a class="reference internal" href="chatgpt.html">ChatGPT ‘Real-Life Example’</a></li>
<li class="toctree-l1"><a class="reference internal" href="mltensorflow.html">Example : Higgs Boson</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Differential equations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="runge_kutta.html">Initial value problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="boundary_value_problems.html">Boundary value problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="symplectic.html">Symplectic integrators</a></li>
<li class="toctree-l1"><a class="reference internal" href="laplace.html">Poisson’s equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="conjugate_gradient.html">Sparse matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_instability.html">Boundary conditions and numerical stability</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffusion.html">Diffusion in multi-dimensions</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="ode_solutions.html">Solutions to exercises</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="runge_kutta_solutions.html">Initial value problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="bvps.html">Boundary value problems</a></li>
<li class="toctree-l2"><a class="reference internal" href="symplectic_solutions.html">Symplectic integrators</a></li>
<li class="toctree-l2"><a class="reference internal" href="conjugate_gradient_solutions.html">Conjugate gradient solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="numerical_instability_solutions.html">Boundary conditions and numerical stability solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="adi.html">Alternating-direction implicit</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Spectral methods</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="fft.html">Discrete Fourier transform</a></li>
<li class="toctree-l1"><a class="reference internal" href="advect_diffuse.html">Advection-diffusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_diffusion.html">Amplitude and phase errors</a></li>
<li class="toctree-l1"><a class="reference internal" href="burgers.html">Burgers’ equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="signal_processing.html">Signal processing</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="spectral_solutions.html">Solutions to exercises</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="fft_solutions.html">Discrete Fourier Transform</a></li>
<li class="toctree-l2"><a class="reference internal" href="fft_1d_solutions.html">More on the 1D DFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="adv_diff_fourier.html">Advection-diffusion with Fourier</a></li>
<li class="toctree-l2"><a class="reference internal" href="adv_diff_finite_difference.html">Advection-diffusion with finite differences</a></li>
<li class="toctree-l2"><a class="reference internal" href="LIGO.html">LIGO tutorial: find an inspiral</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Performance</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="performance.html">Timing, profiling and compiling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Summary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="summary.html">Summary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Homework</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="hw1.html">Homework 1</a></li>
<li class="toctree-l1"><a class="reference internal" href="hw2.html">Homework 2</a></li>
<li class="toctree-l1"><a class="reference internal" href="hw3.html">Homework 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="hw4.html">Homework 4</a></li>
<li class="toctree-l1"><a class="reference internal" href="hw5.html">Homework 5</a></li>
<li class="toctree-l1"><a class="reference internal" href="hw6.html">Homework 6</a></li>
<li class="toctree-l1"><a class="reference internal" href="hw7.html">Homework 7</a></li>
<li class="toctree-l1"><a class="reference internal" href="exam.html">Exam questions and solutions</a></li>

<li class="toctree-l1"><a class="reference internal" href="project_instructions.html">Project Instructions</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="homework_solutions.html">Homework solutions</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="HW1_solutions.html">Homework 1 solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="HW2_solutions.html">Homework 2 solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="homework_notes/hw2_common_issues.html">Homework 2: Common Issues</a></li>














<li class="toctree-l2"><a class="reference internal" href="HW3_solutions.html">Homework 3 solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="HW4_solutions.html">Homework 4 solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="HW5_solutions.html">Homework 5 solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="HW6_solutions.html">Homework 6 solutions</a></li>
<li class="toctree-l2"><a class="reference internal" href="HW7_solutions.html">Homework 7 solutions</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/andrewcumming/phys512" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/andrewcumming/phys512/issues/new?title=Issue%20on%20page%20%2Fmllecture.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/mllecture.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state-of-the-field">State of the Field</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baff907abd05817b0b818e6f215-png"></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-and-supervised-learning">Unsupervised and Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-basics">ML Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkers">Checkers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#side-note-on-choices-in-numerical-physics">Side note on Choices in Numerical Physics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkers-part-2">Checkers Part 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specific-examples">Specific Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principle-component-analysis">Principle Component Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specific-neural-networks">Specific Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ai-coding-assistance">Using AI Coding Assistance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt">ChatGPT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#github-copilot">Github Copilot</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#books">Books:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introductory-links">Introductory Links:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specific-topics">Specific Topics:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">Misc:</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="machine-learning">
<h1>Machine Learning<a class="headerlink" href="#machine-learning" title="Permalink to this heading">#</a></h1>
<p>By <a class="reference external" href="https://github.com/matthew-w-lundy">Matthew Lundy</a></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>When it comes to machine learning we are dealing with a very broad and active field so it’s often difficult to fully encompass all that falls under this umbrella. First off, it’s important to discuss what we mean by certain terms. I often find that core concepts in <a class="reference external" href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> (ML) are familiar to folks, or easy to understand but people just have issues with the terminology. This is going to be a mix of the core concepts that cause ML to work as well as well as some introduction to the theory. This work relies heavily on <a class="reference external" href="http://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf">Mitchell</a> (2017) as a key reference. We will try to summarise the state of the field and provide brief introduction to neural networks.</p>
<p>When discussing ML a lot of terms are used interchangeably, often because they are subsets of the larger. Here are a couple of key terms you may have heard used in the past along with the working definitions we will be using:</p>
<ul class="simple">
<li><p><strong>Artificial</strong> <strong>Intelligence</strong> <strong>(AI):</strong> This broad category encompasses any task solved by a computer or robot that would normally require human intervention.</p></li>
<li><p><strong>Machine Learning (ML):</strong> These are a series of recursive techniques where a system improves its performance based on information applied to it.</p></li>
<li><p><strong>Neural Networks (NN):</strong> These are a specific flexible functional form capable of fitting a large set of functions. Often ML is optimising a neural network.</p></li>
<li><p><strong>Deep Learning (DL):</strong> These are a specific set of NN which have more “layers”. This will be explained in a later section.</p></li>
</ul>
</section>
<section id="state-of-the-field">
<h2>State of the Field<a class="headerlink" href="#state-of-the-field" title="Permalink to this heading">#</a></h2>
<p>A lot of machine learning is getting easier to perform. Performance statistics, not understanding, is the goal of most of these algorithms which means that many of the key techniques in machine learning do not have a good theoretical background. Often times the less “practical” guides just introduce some matrix algebra and less pictures, they are equally practical. Some techniques are still open questions so finding proofs for those are very challenging and aren’t rigorously defined from first principles but instead arise from trial and error.</p>
<p>In many scenarios machine learning is more effective than traditional algorithms. In most cases, when you are only focused on performance then machine learning has a place.</p>
<p>As you may have seen now throughout the course, the vast majority of practical implementations of code that you use will come from preexisting packages. Even though you have learned <a class="reference external" href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">Levenberg–Marquardt</a>  you are still more likely to use <code class="docutils literal notranslate"><span class="pre">scipy.curvefit</span></code> when you approach a problem. In machine learning this is often taken to the extreme where you will use many components that you may not fully understand every detail of but can still produce powerful results. Hopefully through this lecture you may be able to understand at least the broad scope of what the packages are doing and where to start to find resources to understand every component.</p>
<p>Unlike <code class="docutils literal notranslate"><span class="pre">scipy</span></code> and <code class="docutils literal notranslate"><span class="pre">numpy</span></code> there isn’t a single accepted <em>framework</em> for machine learning. Instead there are multiple competing frameworks that can be used. Most of these are backed by large corporations but that means they are well supported and funded. Most ML work is done in these packages in both academia and outside. Once you learn the basics of one of these frameworks it’s very easy to move and change your workflow to the others. Unfortunately, I will be teaching with <a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a>. This framework is backed by Google, but is currently becoming less popular in academia. The following shows the popularity of frameworks with papers with Github repositories. <a class="reference external" href="https://pytorch.org/">PyTorch</a> is becoming more popular as it is more <em>pythonic</em> and easier to debug.</p>
<p><img alt="code.png" src="_images/code.png" /></p>
<p>There are a couple of indicators though that <code class="docutils literal notranslate"><span class="pre">Tensorflow</span></code> may have a persistent userbase even if growth has slowed. There continues to be regular traffic in Google (as seen by <a class="reference external" href="https://trends.google.com/trends/explore?date=all&amp;q=tensorflow,pytorch&amp;hl=en">Google Trends</a>) for <code class="docutils literal notranslate"><span class="pre">Tensorflow</span></code> although even there <code class="docutils literal notranslate"><span class="pre">PyTorch</span></code> appears to be overtaking the older framework.</p>
</section>
<section id="baff907abd05817b0b818e6f215-png">
<h2><img alt="82736baff907abd05817b0b818e6f215.png" src="_images/82736baff907abd05817b0b818e6f215.png" /><a class="headerlink" href="#baff907abd05817b0b818e6f215-png" title="Permalink to this heading">#</a></h2>
<p>There has been a huge increase though in the last 10 years of the adoption and use of ML in a variety of applications mostly due to the development of these packages. The only other package that I will mention at this moment is <em><a class="reference external" href="https://root.cern/manual/tmva/">Toolkit for Multivariate Data Analysis</a> (TMVA)</em> in <code class="docutils literal notranslate"><span class="pre">ROOT</span></code>.  In many physics applications ROOT is still the dominant language as much of the legacy software was written in ROOT and so when applying ML to problems involving these datasets it’s easier to remain in the <code class="docutils literal notranslate"><span class="pre">ROOT</span></code> framework.</p>
</section>
<section id="unsupervised-and-supervised-learning">
<h2>Unsupervised and Supervised Learning<a class="headerlink" href="#unsupervised-and-supervised-learning" title="Permalink to this heading">#</a></h2>
<p>In physics applications it’s often useful to separate our problems into two scenarios based on the data that we have. These are <em>supervised learning</em> and <em>unsupervised learning</em>. We consider a supervised dataset as being one where for every data value we have a corresponding <em>label</em> or <em>true parameter</em> that we are seeking to often reproduce. We can represent this in simple set notation as:</p>
<p>Supervised Dataset <span class="math notranslate nohighlight">\(\{(x[i],d[i])\}_{i=1,2,3,....}\)</span></p>
<p>Consider a <em>classification</em> problem, one where <span class="math notranslate nohighlight">\(d\)</span> in this case could be some form of category. The other form of dataset would be an unsupervised dataset. With the same form this would look something like:</p>
<p>Unsupervised Dataset <span class="math notranslate nohighlight">\(\{(x[i],\_\_\}_{i=1,2,3,....}\)</span></p>
<p>You can see that these datasets are clearly less helpful than the supervised training sets but often closer to what we might be able to collect in reality. We may not know the underlying characterisation of the data, we also may not have a complete picture of the data and so our classifications in <span class="math notranslate nohighlight">\(d\)</span> might be incorrect.</p>
<p>A common physics application is <strong>particle identification</strong>, is this track a muon, an electron, or a dark matter particle? These will often also have unique shapes, impacts, and even sounds (see PICO). In the early days of particle physics (~1900s), one of the most useful particle detectors was the cloud chamber. These produce a trail of condensed gas along the path of ionisation caused by particle passing through the gas. The different masses and charges allowed for particle physicists to distinguish between different particles leading to many new discoveries. See below for a guide to the way that different particles behave in a cloud chamber:</p>
<p><img alt="cloudchambereffects.png" src="_images/cloudchambereffects.png" /></p>
<p>By default, a cloud chamber is an unsupervised environment. Passive cosmic rays impact the cloud chamber and don’t label themselves after they’ve interacted. Although this is the case, you can convert unsupervised into an supervised dataset with a lot of work. If you were to try and create a supervised dataset for a cloud chamber there are multiple ways you could approach this problem.</p>
<ul class="simple">
<li><p><strong>Inject Known Signals</strong>: Having radiation source for example could provide a large sample of beta or alpha particles which you could image and then create a set of data where the vast majority is a single particle type.</p></li>
<li><p><strong>Simulate Images</strong>: Take the known shapes, and either parameterise their forms or fully run through particle simulation in a software like GEANT4. This could artificially generate images that match the data and you would known exactly what type of particle was being produced by the simulation.</p></li>
<li><p><strong>Manually Label</strong>: The labelling of data could be done by hand. You could go through with your reference sheet some subset of images and generate the labelling manually or pseudo manually. This does require a lot of graduate student labor.</p></li>
</ul>
<p>So overall it’s often a very difficult process to generate an adequate training sample for supervised learning applications. In tutorials/online guides this process is often skipped and you are provided with a fully labelled dataset. Often in problems you will spend over 80% of your time generating the supervised training set</p>
<p>Now both of these scenarios often come up in physics. It’s both often useful to attempt to extract “new information” from a large data set and also important to be able to extract known useful information reliably and quickly with an unknown function underneath. There are often a couple of sub fields that are discussed, when your <span class="math notranslate nohighlight">\(d\)</span> only contains a set of target values, we often consider this a being a <em>classification</em> problem. In the continuous case this is called <em>regression</em>.</p>
<p>But all of these methods still require a deep understanding of your underlying sample in order to produce the labels. What if you have data with no labels? These <em>unsupervised</em> data sets can still be useful. If you are dealing with a set where you trying to sort the data into distinct categories then we call this <em>categorisation</em>. This is often useful when you know that your representation of the data aligns closely along its intrinsic properties. If it doesn’t, often times the case when collecting raw data the things that are simple to measure may not be the intrinsic properties by which the system clusters, then you may want to try dimensionality reduction. Below is a map which illustrates how one may take the data set they have and begin to approach a machine learning problem in a <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> framework.</p>
<p><img alt="Screenshot2023-10-16160029.png" src="_images/Screenshot2023-10-16160029.png" /></p>
<p>Let’s look at an example based on the above mentioned particle classification in a bubble chamber trained by somebody else: <a class="reference external" href="https://www.youtube.com/watch?v=7ShRrDpjNQk">https://www.youtube.com/watch?v=7ShRrDpjNQk</a> . In this case the labelling was done manually.</p>
<p>In practice, this is used in VERITAS to separate gamma-rays and hadronic particles based on the images obtained after extensive air showers. Visually one can see a couple of clear features that are eventually picked up by the training. This work is based on a massive simulation effort to create a labelled training set that is optimized on. Current work is ongoing to improve our point spread function and sensitivity based on this work. There is also work being done by Maryn Askew (in this class) to asses improvements from using different frameworks. Supervised datsets have also been created for muon identification in VERITAS through manual labelling but by outsourcing the labour the labelling process to the public.</p>
<div class="admonition-example-muon-hunter admonition">
<p class="admonition-title">Example: Muon Hunter</p>
<p>Visit <a class="reference external" href="https://www.zooniverse.org/projects/zooniverse/muon-hunter-classic/">Muon Hunter</a> a “Zooniverse” project. Either select “Muon or Not?” or “Draw a Ring” and complete the tutorial. Classify a couple of examples. See how the data is being presented in way that makes the problem tractable for people with little knowledge of the problem. Check out <a class="reference external" href="https://www.zooniverse.org/projects?discipline=physics&amp;page=1&amp;sort=-classifiers_count&amp;status=live">Zooniverse’s</a> other project and look through a couple of the other physics related projects.</p>
</div>
</section>
<section id="ml-basics">
<h2>ML Basics<a class="headerlink" href="#ml-basics" title="Permalink to this heading">#</a></h2>
<p>So let’s start from the beginning of a complicated problem and figure out how somebody would think about an issue in an ML framework. We will use the Mitchell definition of ML which is as follows:</p>
<p><code class="docutils literal notranslate"><span class="pre">&quot;A</span> <span class="pre">computer</span> <span class="pre">program</span> <span class="pre">is</span> <span class="pre">said</span> <span class="pre">to</span> <span class="pre">learn</span> <span class="pre">from</span> <span class="pre">experience</span> <span class="pre">E</span> <span class="pre">with</span> <span class="pre">respect</span> <span class="pre">to</span> <span class="pre">some</span> <span class="pre">class</span> <span class="pre">of</span> <span class="pre">tasks</span> <span class="pre">T</span> <span class="pre">and</span> <span class="pre">performance</span> <span class="pre">measure</span> <span class="pre">P,</span> <span class="pre">if</span> <span class="pre">its</span> <span class="pre">performance</span> <span class="pre">at</span> <span class="pre">tasks</span> <span class="pre">in</span> <span class="pre">T,</span> <span class="pre">as</span> <span class="pre">measured</span> <span class="pre">by</span> <span class="pre">P,</span> <span class="pre">improves</span> <span class="pre">with</span> <span class="pre">experience</span> <span class="pre">E.”</span> <span class="pre">(Mitchell,</span> <span class="pre">1997)</span></code></p>
<p>Starting with <strong>tasks</strong>, a task is what the computer is actually doing. This set of actions can be very simple or very arduous. Often times when using ML it’s easy to lose track of what you actually are trying to accomplish. You should clearly lay out your task for what you want from your tool to do at the beginning.</p>
<p>Some examples of common tasks include speech recognition, driving autonomous cars, astronomical classification, and playing backgammon. ML excels at some tasks but can often still perform pretty poorly in certain scenarios, the safest way to proceed is by seeing if this has been implemented in similar environments and expand slowly. There is other reasons to do this as well including the common practice of <em>bootstrapping</em> where you take preexisting networks trained on large datasets and insert them into your own training such that you do not need to start from scratch. It’s becoming more of a regular practice in physics to make your code available after the publication of a paper which should continue to make this process easier in the future.</p>
<section id="checkers">
<h3>Checkers<a class="headerlink" href="#checkers" title="Permalink to this heading">#</a></h3>
<p>For our example of a task we will use the very simple <strong>“playing a game of checkers”.</strong> For those unfamiliar the rules of checkers they are very simple, you move pieces diagonally forward 1 square at a time unless the square is occupied by an opponents piece in which case (if there is empty space) you can capture the “piece” by jumping over it. You remove the captured the pieces. When you get to the last row you transform into a “king” which will allow the piece to also move backwards. The winner is decided when only pieces from one colour remain.</p>
<p><img alt="deid.png" src="_images/deid.png" /></p>
<p><strong>Performance</strong> measure is how you asses how well you are doing at the given task. This may seem easy but depending on how you formulate this, there can be a large discrepancies in your results. This measure of performance is often given many names depending on the precise discipline. This is often slightly different to what you use to actually train your data. For our example we would may not train our system by playing full games of checkers against an opponent but we would measure the success of our network based on those outcomes. The parameter than we use to improve our network is called the <em>loss function</em>. You will see later that you’ve already encountered something very similar to a <em>loss function</em>.</p>
<p>A well constructed <em>loss function</em> will be minimised as performance is maximised. Often times the performance is simply measured by the changes in this loss function.</p>
<p><strong>Experience</strong> is how we explore the parameter space of our loss function. It’s how we get more information to asses how changes in task performance affect the loss function.</p>
<p>Gaining more experience in many environments is called <em>training</em>. Depending on your task this often involves millions of simulations. Real data is sometimes used but often for the requirements of ML you need to use large datasets. There is often a heavy computational or observational burden at this stage and it’s often very unique to your particular problem. As such, resources for generating data sets to use in ML applications is often very sparse. For our checkers example we could use “games played against itself”.</p>
<p>This is the first of many choices that are made for this problem that carry with them some issues. In this case we run into a major issue of <em>generalisation error</em>. This arises when the data that we use for optimisation does not adequately represent the data that we are assessing the performance against. In this case the ability to perform well in games against an AI may not be the same as the ability to perform well against a human, which may be a metric that we are interested in measuring. If the AI only plays itself, there may be scenarios that are under weighted that a checkers champion may know by heart. Often generalisation error is only measured and there are earnest attempts to reduce the error at the early stages when the experience data set is being generated.</p>
</section>
<section id="side-note-on-choices-in-numerical-physics">
<h3>Side note on Choices in Numerical Physics<a class="headerlink" href="#side-note-on-choices-in-numerical-physics" title="Permalink to this heading">#</a></h3>
<p>Numerical methods is often filled with many choices with no particular right answer, simply the answer that seems best for your problem. In many classes prior, there will be a “right” way of coming to a solution. You are told to use the Lagrangian to answer this problem, or you are told to not use Gauss’s law. In this class as well you will encounter well formulated problems that often take away from you the option to make choices. This assists greatly in assessing the ability of individual students however it does make it such that you will often not find yourself in the difficult position of making choices until you are well into your research or in industry. Depending on your supervisor you may not find yourself even making significant choices until you are more senior. The ability to select the right system to tackle a problem is just as important as the ability to implement that system well. I would encourage you in this class to recognise the tools put before you and make a note of when a choice is being made for you. Ask yourself why these choices are being made and do not take for granted that it’s the best way to tackle a problem.</p>
</section>
<section id="checkers-part-2">
<h3>Checkers Part 2<a class="headerlink" href="#checkers-part-2" title="Permalink to this heading">#</a></h3>
<p>Let’s think more about this checkers problem and how we would go about designing a system. Specifically how we might chose to tackle the issue of of performance and loss functions. We know that we are simulating games of checkers, and we can tell if the system has lost or won based on the rules of the game but is this situation sufficient for training our system. Can we simply input a list of all board states in a game into the AI with a loss or a win and expect this system to be optimised? Well it depends but likely this will result in major problems.</p>
<p>Imagine the following scenario, what if during this simulated games the computer plays the best opening ever but loses due to some bad moves towards just the final half? To our system this would look the exact same as a game where every move was awful. This would result in a large amount of wasted information and also it will be more confusing for the system to understand when it is making good choices.</p>
<p>This is the problem of <strong>credit assignment.</strong> In cases where the feedback given is indirect (like in our example), how you determine what was the problem is hard. We could in principle asses instead individual checkers board states and do a move by move check, this would be more direct form feedback. But this also carries with it major issues as we will see.</p>
<p>Although our final performance metric is percent of games won, this not what we optimise. It is often best to reduce the problem of improving performance <span class="math notranslate nohighlight">\(P\)</span> to task <span class="math notranslate nohighlight">\(T\)</span> to the problem of learning some target function. In checkers a good target function is a choice of the best move from a set of legal moves. This could be shown as</p>
<p><span class="math notranslate nohighlight">\(ChooseMove: B\rightarrow M\)</span></p>
<p>It turns out though that <span class="math notranslate nohighlight">\(ChooseMove\)</span> may seem like a good definition often times it’s better to define a function <span class="math notranslate nohighlight">\(V\)</span> which instead tells us if a board state is “good” or “bad”.</p>
<p>Let’s call <span class="math notranslate nohighlight">\(V\)</span> our target function, now we need to find a way to define a board state is good. Lets say that <span class="math notranslate nohighlight">\(V(b)\)</span> is <span class="math notranslate nohighlight">\(V\)</span> acting on some board state in <span class="math notranslate nohighlight">\(B\)</span> the space containing all possible board states. One potential defintion could be the piecewise function:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(b\)</span> is a won board state <span class="math notranslate nohighlight">\(V(b)=100\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(b\)</span> is a lost board state <span class="math notranslate nohighlight">\(V(b)=-100\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(b\)</span> is a drawn board state <span class="math notranslate nohighlight">\(V(b)=0\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(b\)</span> is not a final state than <span class="math notranslate nohighlight">\(V(b)\)</span> is the mean of all possible final board states that can be achieved from b</p></li>
</ul>
<p>This is a <em>non operational</em> definition!</p>
<p>In practice this is impossible to compute, so we are looking for an <em>operational</em> target function. This is often only an approximation of <span class="math notranslate nohighlight">\(V\)</span>. So lets define <span class="math notranslate nohighlight">\(V^*\)</span> as the operational approximation of <span class="math notranslate nohighlight">\(V\)</span>. This could be a lot of things including a neural network, but let’s start simple by using a linear combination of board features.</p>
<ul class="simple">
<li><p><em><span class="math notranslate nohighlight">\(x1\)</span> : the number of black pieces on the board</em></p></li>
<li><p><em><span class="math notranslate nohighlight">\(x2\)</span> : the number of red pieces on the board</em></p></li>
<li><p><em><span class="math notranslate nohighlight">\(x3\)</span> : the number of black kings on the board</em></p></li>
<li><p><em><span class="math notranslate nohighlight">\(x4\)</span> : the number of red kings on the board</em></p></li>
<li><p><em><span class="math notranslate nohighlight">\(x5\)</span> : the number of black pieces threatened</em></p></li>
<li><p><em><span class="math notranslate nohighlight">\(x6\)</span> : the number of red pieces threatened</em></p></li>
</ul>
<p>Having this target function we’ve now setup most of the problem. Below is a summary of this setup:</p>
<hr class="docutils" />
<p>T: Play Checker
P: Percent of games won against opponents
E: Games played against itself</p>
<p>Target Function : <span class="math notranslate nohighlight">\(V : Board\)</span>
Target function approximation:
<span class="math notranslate nohighlight">\(V^*(b)= w0+w1x1+w2x2+w3x3+w4x4+w5x5+w6x6\)</span></p>
<hr class="docutils" />
<p>We have changed the problem from “learn checkers” which seems pretty daunting, to “learn 6 coefficients”. This simplification makes the problem much more tractable</p>
<p><span class="math notranslate nohighlight">\(V^*\)</span> is still hard to figure out though. We need some way of converting a win and a loss into some measurement of an individual board state without calculating all possible results.  What we want to give the code is some board state and an paired true V value such that you have <span class="math notranslate nohighlight">\(&lt;b,V_{train}(b)&gt;\)</span>.</p>
<p>An example would be
<span class="math notranslate nohighlight">\(&lt;x1=3, x2=0, x3=1, x4=0, x5=0, x6=0,+100&gt;\)</span></p>
<p>The only information we have is all the moves played and whether the game was won or lost. To translate this into a measure of intermediate board states we can assign the training value to be approximately the value of the <span class="math notranslate nohighlight">\(V_{train}(b)\sim V^*(Successor(b))\)</span> (the state after both players have moved subsequent to this one). This may seem a bit confusing but <span class="math notranslate nohighlight">\(V^*\)</span> should begin to approach <span class="math notranslate nohighlight">\(V\)</span> through minimization. There is a more rigorous proof of this under certain assumptions.</p>
<p>Now we can have a set of “measurements” or training examples <span class="math notranslate nohighlight">\(\{&lt;b,V_{train}(b)&gt;\}\)</span> and we can adjust our weights till this is “good”. We would like to update our weights more rapidly based on how wrong the system is as well so we can define an error as being:</p>
<p><span class="math notranslate nohighlight">\(E=\sum_{&lt;b,V_{train}(b)&gt;}(V_{train}(b)-V^*(b))^2\)</span></p>
<p>These errors should be familiar to you know as should be the method of iteratively reducing. For each training example we will perform the following:</p>
<ul class="simple">
<li><p>Use the current weights to calculate <span class="math notranslate nohighlight">\(V^*(b)\)</span></p></li>
<li><p>For each weight update it by <span class="math notranslate nohighlight">\(η(Vtrain(b) – V*(b))\)</span>x where <span class="math notranslate nohighlight">\(η\)</span> is a small constant.</p></li>
</ul>
<p>And that’s all that’s required for finishing the setup of this machine learning problem. Coding this specific example would be quite a significant task but the framework for going through and setting up a machine learning problem should be clearer know. Mostly what you are trying to do is reduce a complicated problem into a situation where you are solving a linear equation again. This may not seem exceptionally satisfying but as you will see, having a flexible solver solve a flexible underlying function is all you need to solve some really difficul problem. Below is a flow chart that shows the decisions that were made as a part of this process.</p>
<p><img alt="Screenshot2023-10-16145059.png" src="_images/Screenshot2023-10-16145059.png" /></p>
</section>
</section>
<section id="specific-examples">
<h2>Specific Examples<a class="headerlink" href="#specific-examples" title="Permalink to this heading">#</a></h2>
<section id="principle-component-analysis">
<h3>Principle Component Analysis<a class="headerlink" href="#principle-component-analysis" title="Permalink to this heading">#</a></h3>
<p>PCA is the only form of unsupervised learning that will be discussed as it is often used in other forms of machine learning in order to understand the output. PCA is a form of dimensionality reduction where we seek to find the true principle axes of the underlying data and apply a transformation along the axes with the largest impact of the scatter of the data. PCA falls naturally out of SVD as PCA is simply the process of extracting the eigenvectors of the problem a reprojecting the variables onto those new axes. From the previous class we know any matrix there always exists a <span class="math notranslate nohighlight">\(U,S,V\)</span> such that we can write</p>
<p><span class="math notranslate nohighlight">\(X=USV^T\)</span></p>
<p>with <span class="math notranslate nohighlight">\(U,V\)</span> being orthongonal and <span class="math notranslate nohighlight">\(S\)</span> being diagonal. Now the covariance of a matrix is given by <span class="math notranslate nohighlight">\(C=\frac{XX^T}{n-1}\)</span>. This is a square symmetric matrix where the variances lies on the diagonal and the covariances (measured of how often two datasets move in the same direction) lie of the off diagonal entries. Covariance matrices are extremely useful for understanding the relationship that different parts of you data have with other components. The eignevectors of this matrix are going to be the lines by which the data tends to vary. So substituting our SVD of <span class="math notranslate nohighlight">\(X\)</span> into this defintion we have:</p>
<p><span class="math notranslate nohighlight">\(C=\frac{XX^T}{n-1}=\frac{VS^TU^TUSV^T}{n-1}=V\frac{S^2}{n-1}V^T\)</span></p>
<p>Thus the <span class="math notranslate nohighlight">\(V\)</span> that we obtain from the SVD of <span class="math notranslate nohighlight">\(X\)</span> is a othronogonal matrix whose columns are the <strong>eigenvector</strong> of the covariance matrix, whose corresponding <strong>eigenvalues</strong> are in the diagonal of <em>S</em>.</p>
<div class="admonition-example-svd-to-pca admonition">
<p class="admonition-title">Example: SVD to PCA</p>
<p>Write a code that randomly generates 100 two dimensional points following a linear function from 0 to 10 with slope 1. Add a Gaussian scatter to these points in y with sigma = 1. Center this data back to zero (the equations rely on this assumption). Add these all into a single vector and calculate single value decomposition. What is the form of V? Plot the two directions of the vectors in V, do these line up with your expectations?</p>
</div>
</section>
<section id="neural-networks">
<h3>Neural Networks<a class="headerlink" href="#neural-networks" title="Permalink to this heading">#</a></h3>
<p>Neural networks, named after the neuron in the brain. These are functions created through dense sets of interconnected simple units that take an abitrary set of real inputs and translter them into a series of real value ouput. This is similar to the brain in the way we are run based on a series of small neuron activations. One of the fundamental units of the NN is the perceptron. These take the values from a series of real value input and pass the weighted sum of these quantieis into a function that return a value of -1 to 1. Perceptrons can easily construct boolean functions such that any abitrary function can be constructed out of these fundamentals. The process of developing complicated networks will simply involve construction complicated versions of these simple systems.</p>
<p>From Mitchell (1997):
<img alt="2ee3f414af54bdb362bf9f15034c4e6e.png" src="_images/2ee3f414af54bdb362bf9f15034c4e6e.png" /></p>
<div class="admonition-example-building-a-perceptron admonition">
<p class="admonition-title">Example: Building a Perceptron</p>
<p>We build a neural network with a single node that takes three inputs and ouputs a single value. Define a sigmoidal function in python. Define as well the gradient of the sigmoidal function, we will see later in the course the importance of derviatives in optimization but for now it’s simply a measure of the confidence of the existing weights. Initilize a weights vector with values between -1 and 1.</p>
<p>Create a loop that performs the following n times:
Caclulate the dot product between the weights and the input.
Pass the dot product to the sigmoidal function.
Compare the output to the desired ouptut.
Update the weights by taking the dot product of the input and the error multiplied by the derivative of the sigmoid (add this).</p>
<p>Create a training set where if all the values in the input matrix are positive it’s +1 and if any are negative it’s 0.</p>
<p>Run the network.</p>
</div>
</section>
<section id="specific-neural-networks">
<h3>Specific Neural Networks<a class="headerlink" href="#specific-neural-networks" title="Permalink to this heading">#</a></h3>
<p>Here I will briefly touch on different common architectures that are discussed when people talk about ML applications. I will not describe them in detail just highlight when they are used and broadly how they work in principle. In order to understand them you first need to understand that every layer in a neural network doesn’t need to be the same and also does not need to follow the simple outline above. Often these simple layers are called Dense layers, where every neuron is fully connected. One of the most common other layers is a convolutional layer where a dynamic filter is applied to the data often to smooth and down sample the data to key features. This is often used when information from the spatial realtionship of neighbouring entries is important (like an image). Pooling is a simple version of something similar where the average or maximum of a set of values is taken in a series of neighbouring pixels.</p>
<p>The description of all the different layers is called the network architecture and often in papers people will explain the architecture they use to tackle their particular problem. An example <a class="reference external" href="https://www.nature.com/articles/s41524-022-00803-w">paper</a>  by Yao et al. (2022) shows a style of presentation that is often used. Hopefully you can now broadly understand what’s happening at a structural level when you see these plots and it’s significantly less complicated than it looks.</p>
<p><img alt="41524_2022_803_Fig1_HTML.webp" src="_images/41524_2022_803_Fig1_HTML.webp" /></p>
<p>Other common forms of neural networks include recurrent neural networks. These break the unidirectional neural network and feed the input of some nodes to earlier layers. This gives the network a sort of internal memory that makes them very useful for applications where the sequence of the information contains necessary information to understand the subsequent steps.</p>
</section>
</section>
<section id="using-ai-coding-assistance">
<h2>Using AI Coding Assistance<a class="headerlink" href="#using-ai-coding-assistance" title="Permalink to this heading">#</a></h2>
<p>Is it ethical to use AI coding assistance? What’s the process for citing AI coding assistance? What’s the process for citing code at all? These are all deeply interesting questions with no clear answers. The Canadian Institute for Theoretical Astrophysics (CITA) has had multiple talks with so called “GPT Pilots” who were just members with GPT 4 to describe how to integrate and use these tools as a part of your regular work flow. <a class="reference external" href="https://arxiv.org/pdf/2303.17125.pdf">A survey of developers was also conducted</a>. This allows us to learn some of the best uses for these tools as well as places where the tools currently tend to fail.</p>
<p><img alt="cfff42cfd467074a99fcfcb8228da635.png" src="_images/cfff42cfd467074a99fcfcb8228da635.png" /></p>
<p><img alt="b33b173d0113fb716c2de836ce4a1867.png" src="_images/b33b173d0113fb716c2de836ce4a1867.png" /></p>
<p>The two most popular tools are ChatGPT and GitHub Copilot both of which appear to have different and complementary applications to users workflow. ChatGPT is the most popular but users report a smaller fraction of their code being written by ChatGPT when compared to Copilot.</p>
<p>Often the tasks where users have a lot of success are currently repetitive or common tasks, things where you are typically using a well cited stackoverflow page or the example documentation. The most common reason though for giving up on code ouput (43% of the time) was that the output was not able to perform the task that was wanted.</p>
<p>There’s also a couple of specific forms of advice that may be useful.</p>
<ul class="simple">
<li><p>Currently AI works well with “Complete code that is highly repetitive but cannot be copied and pasted directly”</p></li>
<li><p>“It however, fails assisting me when I’m writing a more complex algorithm (if not well known).”</p></li>
<li><p>“Since [the codebase] is a polyglot project with golang, java, and cpp implementations, I benefit a lot from…polyglot support.”</p></li>
<li><p>“I mainly use it to…annotate my code for my colleagues.”</p></li>
<li><p>“Be incredibly specific with the instructions and write them asprecisely as I would for a stupid collaborator.”</p></li>
<li><p>“You have to break down what you’re trying to do and write it in steps, it can’t do too much at once.</p></li>
</ul>
<section id="chatgpt">
<h3>ChatGPT<a class="headerlink" href="#chatgpt" title="Permalink to this heading">#</a></h3>
<p>ChatGPT is web based tool with a simple single line interface that can be found here: <a class="reference external" href="https://chat.openai.com/">https://chat.openai.com/</a>. At the time of writing the public free version is ChatGPT-3 althought GPT-4 is available to paid users. GPT-4 is going to be very exciting for physicists as there is a large implementation of plugins that allow you to do things that ChatGPT has been critized for not being able to do in the past. A Wolfram plugin makes solving equation far simpler and more reliable. The scholar plugin will allow it to pull from recent papers, you could ask it for a summary of recent results in a particular field and it will collect the papers that you want. This is a very easy and fun tool to explore. I find myself using ChatGPT in my work. You can see an example in the “Real Life Example” which was used to produce a trigger for <a class="reference external" href="https://www.physics.mcgill.ca/~veritas/lmxb/">VERITAS</a> based on the summer research work of Justin Lipper.</p>
</section>
<section id="github-copilot">
<h3>Github Copilot<a class="headerlink" href="#github-copilot" title="Permalink to this heading">#</a></h3>
<p>Github Copilot is integrated into the IDE enviroment and will provide suggestions based on the currently written code and comments. It is a paid feature to Github but you may be able to access it for free with your McGill email (<a class="github reference external" href="https://github.com/features/copilot">features/copilot</a>). I find myself using this less, it was a form of code autocomplete and code suggestions but it was often difficult to get it to produce large sections of code when I wanted to and I often found it insufficent for the application that I was trying to cover.</p>
<div class="admonition-example-is-ai-that-good admonition">
<p class="admonition-title">Example: Is AI that good?</p>
<p>Try one of the previous exercises in ChatGPT, can you get it to reproduce the correct answer? Can you “prompt engineer” a solution? Did it take longer or shorter than you expect? What are some common errors?</p>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="further-reading">
<h1>Further Reading<a class="headerlink" href="#further-reading" title="Permalink to this heading">#</a></h1>
<p>If you are interested in discussing how to incorporate machine learning into your work (and you are in the TSI) you can contact Ste O’Brien (who provided a lot of the following links).</p>
<section id="books">
<h2>Books:<a class="headerlink" href="#books" title="Permalink to this heading">#</a></h2>
<p>If you are the kind of physicist who love bra-ket notation, and finds Hamiltonians and statistical mechanics deeply satisfying then I recommend <a class="reference external" href="https://link.springer.com/book/10.1007/978-981-33-6108-9">Deep Learning and Physics</a> by Akinori Tanaka et al. (2021).</p>
<p><a class="reference external" href="http://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf">Machine Learning</a> by Tom M. Mitchell (1997) is a simpler overview of the theoretical topics but provides little modern context or easy to implement examples.</p>
</section>
<section id="introductory-links">
<h2>Introductory Links:<a class="headerlink" href="#introductory-links" title="Permalink to this heading">#</a></h2>
<p>Google Intro ML Slides: <a class="reference external" href="https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit#slide=id.g183f28bdc3_0_82">https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/edit#slide=id.g183f28bdc3_0_82</a></p>
<p>NVIDIA Tutorial: <a class="reference external" href="https://drive.google.com/file/d/11kCA4l9_2ACQtdmQ7dp6Yl4_eddeJYK3/view">https://drive.google.com/file/d/11kCA4l9_2ACQtdmQ7dp6Yl4_eddeJYK3/view</a></p>
<p>CERN Tutorial:  <a class="reference external" href="https://indico.cern.ch/event/1170064/attachments/2479920/4256927/vlimant%5C_CERN-SSL%5C_July22.pdf">https://indico.cern.ch/event/1170064/attachments/2479920/4256927/vlimant\_CERN-SSL\_July22.pdf</a></p>
</section>
<section id="specific-topics">
<h2>Specific Topics:<a class="headerlink" href="#specific-topics" title="Permalink to this heading">#</a></h2>
<p>Random Forest Explainer : <a class="reference external" href="https://mlu-explain.github.io/random-forest/?utm_campaign=intro_to_data_series&amp;utm_source=Data_Elixir">https://mlu-explain.github.io/random-forest/?utm_campaign=intro_to_data_series&amp;utm_source=Data_Elixir</a></p>
<p>K Means Clustering:
<a class="reference external" href="https://k-means-explorable.vercel.app/?utm_campaign=intro_to_data_series&amp;utm_source=Data_Elixir">https://k-means-explorable.vercel.app/?utm_campaign=intro_to_data_series&amp;utm_source=Data_Elixir</a></p>
<p>Neural Networks from Scratch:
<a class="reference external" href="https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65">https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65</a></p>
<p>LLM Explainer:
<a class="reference external" href="https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65">https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65</a></p>
<p>Data Cleaning:
<a class="reference external" href="https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt?utm_campaign=intro_to_data_series&amp;utm_source=Data_Elixir">https://counting.substack.com/p/data-cleaning-is-analysis-not-grunt?utm_campaign=intro_to_data_series&amp;utm_source=Data_Elixir</a></p>
<p>Lecture from CITA on ChatGPT
<a class="reference external" href="https://www.youtube.com/watch?time_continue=3496&amp;v=Sgv0FKTEWK4&amp;embeds_referring_euri=https%3A%2F%2Fwww.cita.utoronto.ca%2F&amp;source_ve_path=Mjg2NjY&amp;feature=emb_logo">https://www.youtube.com/watch?time_continue=3496&amp;v=Sgv0FKTEWK4&amp;embeds_referring_euri=https%3A%2F%2Fwww.cita.utoronto.ca%2F&amp;source_ve_path=Mjg2NjY&amp;feature=emb_logo</a></p>
</section>
<section id="misc">
<h2>Misc:<a class="headerlink" href="#misc" title="Permalink to this heading">#</a></h2>
<p><a class="reference external" href="https://handeaygenli.medium.com/particle-identification-system-in-cloud-chamber-using-yolov5-518d58ad9f8f%5B%5D%28https://www.youtube.com/watch?time_continue=3496&amp;v=Sgv0FKTEWK4&amp;embeds_referring_euri=https%3A%2F%2Fwww.cita.utoronto.ca%2F&amp;source_ve_path=Mjg2NjY&amp;feature=emb_logo%29">https://handeaygenli.medium.com/particle-identification-system-in-cloud-chamber-using-yolov5-518d58ad9f8f[](https://www.youtube.com/watch?time_continue=3496&amp;v=Sgv0FKTEWK4&amp;embeds_referring_euri=https%3A%2F%2Fwww.cita.utoronto.ca%2F&amp;source_ve_path=Mjg2NjY&amp;feature=emb_logo)</a></p>
<p>McGill Cloud Chamber
<img alt="20221128_182023.jpg" src="_images/20221128_182023.jpg" /></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="nonlinear_fit.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lorentzian fit</p>
      </div>
    </a>
    <a class="right-next"
       href="chatgpt.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ChatGPT ‘Real-Life Example’</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#state-of-the-field">State of the Field</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baff907abd05817b0b818e6f215-png"></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-and-supervised-learning">Unsupervised and Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-basics">ML Basics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkers">Checkers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#side-note-on-choices-in-numerical-physics">Side note on Choices in Numerical Physics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkers-part-2">Checkers Part 2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specific-examples">Specific Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principle-component-analysis">Principle Component Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">Neural Networks</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specific-neural-networks">Specific Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ai-coding-assistance">Using AI Coding Assistance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatgpt">ChatGPT</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#github-copilot">Github Copilot</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#further-reading">Further Reading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#books">Books:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introductory-links">Introductory Links:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specific-topics">Specific Topics:</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#misc">Misc:</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By <a href="https://www.physics.mcgill.ca/~cumming/">Andrew Cumming</a>
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023, CC BY SA 4.0.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>